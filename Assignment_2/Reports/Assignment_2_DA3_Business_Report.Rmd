---
title: "Predictive Pricing Model for Apartments in Toronto"
author: "Rauhan Nazir"
date: "2/10/2022"
output: pdf_document

---
```{r include=FALSE}
library(knitr)
  opts_chunk$set(fig.path='figure/graphics-', 
                 cache.path='cache/graphics-', 
                 fig.align='center',
                 external=TRUE,
                 echo=TRUE,
                 warning=FALSE,
                 fig.pos='H'
                )
```


```{r message=FALSE, warning=FALSE, include=FALSE}
# CLEAR MEMORY
rm(list=ls())



# Descriptive statistics and regressions

#install.packages("pdp")
library(tidyverse)
library(caret)
library(skimr)
library(grid)
library(glmnet)
library(stargazer)
library(xtable)
library(directlabels)
library(knitr)
library(cowplot)
library(rattle)
library(ranger)
library(Hmisc)
library(kableExtra)
library(ggcorrplot)
library(fixest)
library(pdp)
library(ggpubr)



source("da_helper_functions.R")
source("theme_bg.R")


toronto_final <- readRDS("toronto_final.RDS") %>%
  mutate_if(is.character, factor)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
######################
# Quick look at data #
######################
glimpse(toronto_final)


# where do we have missing variables now?
to_filter <- sapply(toronto_final, function(x) sum(is.na(x)))
to_filter[to_filter > 0]

#We don't have any missing values now 

#####################################
# Look at some descriptive statistics
#####################################

#How is the average price changing  by `property_type`,?
toronto_final %>%
  group_by(property_type) %>%
  summarise(count=n())


price_vs_property_box <- ggplot(data = toronto_final, aes(x = property_type, y = price)) +
  stat_boxplot(aes(group = property_type), geom = "errorbar", width = 0.3,
               color = c(color[2],color[1],color[3]), size = 0.5, na.rm=T)+
  geom_boxplot(aes(group = property_type),
               color = c(color[2],color[1],color[3]), fill = c(color[2],color[1],color[3]),
               size = 0.5, width = 0.6, alpha = 0.3, na.rm=T, outlier.shape = NA) +
  scale_y_continuous(expand = c(0.01,0.01),limits = c(0,325), breaks = seq(0,350,50)) +
  labs(x = "Property Type",y = "Price (Dollars)")+
  theme_bw()



#The mean prices of all the property types are not the same, which might be a good thing for our predictive models due to the 
#variance 


# Barchart to check how the property type is divided according to the number of people it accomodates  
fig1 <- ggplot(data = toronto_final, aes(x = factor(n_accommodates), color = f_property_type, fill = f_property_type)) +
  geom_bar(alpha=0.6, na.rm=T, width = 0.8) +
  scale_color_manual(name="",
                     values=c(color[2],color[1],color[3],color[4])) +
  scale_fill_manual(name="",
                    values=c(color[2],color[1],color[3],color[4])) +
  labs(x = "Accomodates (Persons)",y = "Frequency")+
  theme_bw() 


# Checking the price distributions of absolute and log prices, to see which one resembles the normal distribution more
# and hence be included in the models

price_hist <- ggplot(toronto_final, aes( x = price)) +
  geom_histogram(aes(y = (..count..)/sum(..count..)),fill = "red", color = "black") +
  theme_bw() +
  scale_y_continuous(labels = label_percent()) +
  ylab("Percent") +
  xlab("Price (Dollars)")
price_hist



ln_price_hist <- ggplot(toronto_final, aes(x = log(price))) +
  geom_histogram(aes(y = (..count..)/sum(..count..)),fill = "red", color = "black") +
  theme_bw() +
  scale_y_continuous(labels = label_percent()) +
  ylab("Percent") +
  xlab("ln(Price, Dollars)")
ln_price_hist

#The absolute price values are more normally distributed, hence there is no need to take log, as it makes the distribution a
# long left tail rather than normal

#This can be seen more clearly by putting them together side by side
price_hist_grid <- ggarrange(
  price_hist,
  ln_price_hist,
  nrow = 1)

#Adding the note about the data filtering we have done before this
annotate_figure(price_hist_grid,bottom =
                  text_grob("Note: Apartments with 2-6 accommodation capacity. Histogram with price < 350 Dollars"))



viz1 <- ggarrange(
  price_vs_property_box,
  fig1,
  nrow = 1)



#The absolute price values are more normally distributed, hence there is no need to take log, as it makes the distribution a
# long left tail rather than normal

#This can be seen more clearly by putting them together side by side
price_hist_grid <- ggarrange(
  price_hist,
  ln_price_hist,
  nrow = 1)

#Adding the note about the data filtering we have done before this
annotate_figure(price_hist_grid,bottom =
                  text_grob("Note: Apartments with 2-6 accommodation capacity. Histogram with price < 350 Dollars"))


```


```{r message=FALSE, warning=FALSE, include=FALSE}
#####################
# Setting up models #
#####################


# Basic Variables
basic_lev  <- c("f_property_type", "n_accommodates", "n_beds",  "n_days_sincelast", "flag_days_sincelast")


# Factorized variables
basic_add <- c("f_bathrooms", "f_bedrooms", "f_neighbourhood_cleansed", "f_minimum_nights", "n_availability_365")


reviews <- c("n_review_scores_rating", "flag_review_scores_rating","f_review_scores_rating",
             "n_number_of_reviews","f_number_of_reviews","n_reviews_per_month","flag_reviews_per_month")

host <- c("d_host_is_superhost", "d_host_identity_verified")

# Dummy variables: Extras -> collect all options and create dummies
dummies <-  grep("^d_.*", names(toronto_final), value = TRUE)  

dummiesdf <- as.data.frame(dummies)
write_csv(dummiesdf,"dummies.csv")

#Exported this file to csv so that I can write the interaction terms more easily and paste them back in the code 
#################################
# Look for interactions         #
#################################

price_diff_by_variables3 <- function(df, factor_var, dummy_var, factor_lab, dummy_lab){
  # Looking for interactions.
  # It is a function it takes 3 arguments: 1) Your dataframe,
  # 2) the factor variable (like room_type)
  # 3)the dummy variable you are interested in (like TV)
  
  # Process your data frame and make a new dataframe which contains the stats
  factor_var <- as.name(factor_var)
  dummy_var <- as.name(dummy_var)
  
  stats <- df %>%
    group_by(!!factor_var, !!dummy_var) %>%
    dplyr::summarize(Mean = mean(price, na.rm=TRUE),
                     se = sd(price)/sqrt(n()))
  
  stats[,2] <- lapply(stats[,2], factor)
  
  ggplot(stats, aes_string(colnames(stats)[1], colnames(stats)[3], fill = colnames(stats)[2]))+
    geom_bar(stat='identity', position = position_dodge(width=0.9), alpha=0.8)+
    geom_errorbar(aes(ymin=Mean-(1.96*se),ymax=Mean+(1.96*se)),
                  position=position_dodge(width = 0.9), width = 0.25)+
    scale_color_manual(name=dummy_lab,
                       values=c(color[2],color[1],color[3],color[4])) +
    scale_fill_manual(name=dummy_lab,
                      values=c(color[2],color[1],color[3],color[4])) +
    ylab('Mean Price')+
    xlab(factor_lab) +
    theme_bg()+
    theme(panel.grid.major=element_blank(),
          panel.grid.minor=element_blank(),
          panel.border=element_blank(),
          axis.line=element_line(),
          legend.position = "top",
          #legend.position = c(0.7, 0.9),
          legend.box = "vertical",
          legend.text = element_text(size = 5),
          legend.title = element_text(size = 5, face = "bold"),
          legend.key.size = unit(x = 0.4, units = "cm")
    )
}



# Plot interactions between room type/property type and all dummies
sapply(dummies, function(x){
  p <- price_diff_by_variables3(toronto_final, "f_property_type", x, "property_type", x)
  print(p)
})



# I am using all of the dummy variables to create interaction terms with the property type as I will use LASSO to eliminate those
# which do not have much significance for our prediction model

interactions <- c("f_property_type*d_gestainlesssteelgasstove",
                  "f_property_type*d_backyard",
                  "f_property_type*d_baking_sheet",
                  "f_property_type*d_barbecue_utensils",
                  "f_property_type*d_bathroom_essentials",
                  "f_property_type*d_bathtub",
                  "f_property_type*d_bed_linens",
                  "f_property_type*d_bedroom_comforts",
                  "f_property_type*d_board_games",
                  "f_property_type*d_bread_maker",
                  "f_property_type*d_building_staff",
                  "f_property_type*d_carbon_monoxide_alarm",
                  "f_property_type*d_cleaning_before_checkout",
                  "f_property_type*d_cleaning_products",
                  "f_property_type*d_cooking_basics",
                  "f_property_type*d_dining_table",
                  "f_property_type*d_dishes_and_silverware",
                  "f_property_type*d_drying_rack_for_clothing",
                  "f_property_type*d_elevator",
                  "f_property_type*d_essentials",
                  "f_property_type*d_ethernet_connection",
                  "f_property_type*d_ev_charger",
                  "f_property_type*d_extra_pillows_and_blankets",
                  "f_property_type*d_fire_extinguisher",
                  "f_property_type*d_fire_pit",
                  "f_property_type*d_first_aid_kit",
                  "f_property_type*d_freezer",
                  "f_property_type*d_game_console",
                  "f_property_type*d_hangers",
                  "f_property_type*d_host_greets_you",
                  "f_property_type*d_hot_tub",
                  "f_property_type*d_hot_water",
                  "f_property_type*d_hot_water_kettle",
                  "f_property_type*d_keypad",
                  "f_property_type*d_lake_access",
                  "f_property_type*d_laundromat_nearby",
                  "f_property_type*d_lock_on_bedroom_door",
                  "f_property_type*d_lockbox",
                  "f_property_type*d_long_term_stays_allowed",
                  "f_property_type*d_luggage_dropoff_allowed",
                  "f_property_type*d_microwave",
                  "f_property_type*d_mini_fridge",
                  "f_property_type*d_mosquito_net",
                  "f_property_type*d_outdoor_dining_area",
                  "f_property_type*d_outdoor_furniture",
                  "f_property_type*d_outlet_covers",
                  "f_property_type*d_pool",
                  "f_property_type*d_pool_table",
                  "f_property_type*d_private_entrance",
                  "f_property_type*d_rice_maker",
                  "f_property_type*d_roomdarkening_shades",
                  "f_property_type*d_safe",
                  "f_property_type*d_security_cameras_on_property",
                  "f_property_type*d_shared_hot_tub",
                  "f_property_type*d_shared_pool",
                  "f_property_type*d_single_level_home",
                  "f_property_type*d_smart_lock",
                  "f_property_type*d_smoke_alarm",
                  "f_property_type*d_toaster",
                  "f_property_type*d_trash_compactor",
                  "f_property_type*d_waterfront",
                  "f_property_type*d_window_guards",
                  "f_property_type*d_wine_glasses",
                  "f_property_type*d_have_kitchen",
                  "f_property_type*d_have_stove",
                  "f_property_type*d_have_oven",
                  "f_property_type*d_have_frige",
                  "f_property_type*d_have_o_machineee_machinecoffee",
                  "f_property_type*d_have_gril",
                  "f_property_type*d_have_freeon_premises",
                  "f_property_type*d_have_freestreet",
                  "f_property_type*d_have_paidon_premisvalet",
                  "f_property_type*d_have_paidoff_premisesselfparkingparking",
                  "f_property_type*d_have_wifiinternet",
                  "f_property_type*d_have_cable",
                  "f_property_type*d_have_tv",
                  "f_property_type*d_have_sound_system",
                  "f_property_type*d_have_shampooconditioner",
                  "f_property_type*d_have_body_soapgel",
                  "f_property_type*d_have_washer",
                  "f_property_type*d_have_dryer",
                  "f_property_type*d_have_iron",
                  "f_property_type*d_have_heating",
                  "f_property_type*d_have_air_condfan",
                  "f_property_type*d_have_balconyterrace",
                  "f_property_type*d_have_garden",
                  "f_property_type*d_have_breakfast",
                  "f_property_type*d_have_workoffice",
                  "f_property_type*d_have_fitnessgym",
                  "f_property_type*d_have_childrenbabycribhighcornerchang",
                  "f_property_type*d_have_fireplace",
                  "f_property_type*d_have_clothing_storage",
                  "f_property_type*d_host_is_superhost",
                  "f_property_type*d_host_identity_verified"
)



```


```{r message=FALSE, warning=FALSE, include=FALSE}

# Create test and train samples #
#################################
# now all stuff runs on training vs test (holdout), alternative: 5-fold CV


# create test and train samples (80% of observations in train sample)
smp_size <- floor(0.8 * nrow(toronto_final))

## K = 5
k_folds <- 5
# Define seed value
seed_val <- 200

train_ids <- sample(seq_len(nrow(toronto_final)), size = smp_size)
toronto_final$train <- 0
toronto_final$train[train_ids] <- 1
# Create train and test sample variables
data_train <- toronto_final %>% filter(train == 1)
data_test <- toronto_final %>% filter(train == 0)

#Building the most complex model to use in LASSO
model4 <- paste0(" ~ ",paste(c(basic_lev, basic_add ,reviews, host, dummies, interactions),collapse = " + "))


# Creating the most complex OLS model to run a LASSO. Here LASSO is being used as a tool to choose predictors

# Set lasso tuning parameters:
# a) basic setup
train_control <- trainControl( method = "cv", number = k_folds)
# b) tell the actual lambda (penalty parameter) to use for lasso
tune_grid     <- expand.grid("alpha" = c(1), "lambda" = seq(0.05, 1, by = 0.05))
# c) create a formula
formula <- formula(paste0("price ", paste(setdiff(model4, "price"), collapse = " + ")))

# Run LASSO
set.seed(seed_val)
lasso_model <- caret::train(formula,
                            data = data_train,
                            method = "glmnet",
                            preProcess = c("center", "scale"),
                            trControl = train_control,
                            tuneGrid = tune_grid,
                            na.action=na.exclude)

glimpse(toronto_final)
# Check the output
lasso_model
# Penalty parameters
lasso_model$bestTune
# Check th optimal lambda parameter
lasso_model$bestTune$lambda
# Check the RMSE curve
plot(lasso_model)

# One can get the coefficients as well
lasso_coeffs <- coef(lasso_model$finalModel, lasso_model$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>%
  rename(coefficient = `s1`)  # the column has a name "1", to be renamed

print(lasso_coeffs)

# Check the number of variables which actually has coefficients other than 0
lasso_coeffs_nz<-lasso_coeffs %>%
  filter(coefficient!=0)
print(nrow(lasso_coeffs_nz))
lasso_coeffs_nz

write_csv(lasso_coeffs_nz,"NonZeroCoefficients.csv")

# Get the RMSE of the Lasso model
#   Note you should compare this to the test RMSE
lasso_fitstats <- lasso_model$results %>%
  filter(lambda == lasso_model$bestTune$lambda)
lasso_fitstats

# Create an auxiliary tibble
lasso_add <- tibble(Model='LASSO', Coefficients=nrow(lasso_coeffs_nz),
                    R_squared=lasso_fitstats$Rsquared, BIC = NA,
                    Training_RMSE = NA, Test_RMSE = lasso_fitstats$RMSE )
```

```{r message=FALSE, warning=FALSE, include=FALSE}
toronto_final$d_have_g

## Interaction
p1 <- price_diff_by_variables2(toronto_final, "f_property_type", "d_have_gril","Property Type", "Have Grill") 
p2 <- price_diff_by_variables2(toronto_final, "f_property_type", "d_smoke_alarm","Property Type", "Have Smoke Alarm")
p3 <- price_diff_by_variables2(toronto_final, "f_property_type", "d_shared_pool","Property Type", "Shared Pool")
p4 <- price_diff_by_variables2(toronto_final, "f_property_type", "d_shared_hot_tub","Property Type", "Shared hot Tub") # <------------
p5 <- price_diff_by_variables2(toronto_final, "f_property_type", "d_security_cameras_on_property","Property Type", "Security Camera")

p6 <- price_diff_by_variables2(toronto_final, "f_property_type", "d_mini_fridge","Property Type", "Have Mini fridge")

g_interactions <- plot_grid(p1, p2, p3,
                            p4, p5, p6, nrow=3, ncol=2)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
#Now will update the variables that we are going to used in the model, according to LASSO


# Basic Variables

interactions <- c("f_property_type*d_backyard",
                  "f_property_type*d_baking_sheet",
                  "f_property_type*d_bathroom_essentials",
                  "f_property_type*d_bedroom_comforts",
                  "f_property_type*d_board_games",
                  "f_property_type*d_bread_maker",
                  "f_property_type*d_building_staff",
                  "f_property_type*d_cleaning_before_checkout",
                  "f_property_type*d_cleaning_products",
                  "f_property_type*d_cooking_basics",
                  "f_property_type*d_dishes_and_silverware",
                  "f_property_type*d_drying_rack_for_clothing",
                  "f_property_type*d_ethernet_connection",
                  "f_property_type*d_ev_charger",
                  "f_property_type*d_fire_extinguisher",
                  "f_property_type*d_first_aid_kit",
                  "f_property_type*d_game_console",
                  "f_property_type*d_hot_tub",
                  "f_property_type*d_hot_water_kettle",
                  "f_property_type*d_keypad",
                  "f_property_type*d_lake_access",
                  "f_property_type*d_laundromat_nearby",
                  "f_property_type*d_lockbox",
                  "f_property_type*d_luggage_dropoff_allowed",
                  "f_property_type*d_microwave",
                  "f_property_type*d_mini_fridge",
                  "f_property_type*d_mosquito_net",
                  "f_property_type*d_outdoor_dining_area",
                  "f_property_type*d_outdoor_furniture",
                  "f_property_type*d_outlet_covers",
                  "f_property_type*d_pool",
                  "f_property_type*d_rice_maker",
                  "f_property_type*d_shared_pool",
                  "f_property_type*d_smart_lock",
                  "f_property_type*d_trash_compactor",
                  "f_property_type*d_waterfront",
                  "f_property_type*d_have_gril",
                  "f_property_type*d_have_freeon_premises",
                  "f_property_type*d_have_freestreet",
                  "f_property_type*d_have_paidon_premisvalet",
                  "f_property_type*d_have_tv",
                  "f_property_type*d_have_sound_system",
                  "f_property_type*d_have_washer",
                  "f_property_type*d_have_air_condfan",
                  "f_property_type*d_have_garden",
                  "f_property_type*d_have_childrenbabycribhighcornerchang",
                  "f_property_type*d_have_fireplace",
                  "f_property_type*d_have_clothing_storage",
                  "f_property_type*d_host_greets_you",
                  "f_property_type*d_host_is_superhost"
                  
)

dummies <-c("d_backyard",
            "d_bathroom_essentials",
            "d_bedroom_comforts",
            "d_board_games",
            "d_bread_maker",
            "d_cooking_basics",
            "d_dishes_and_silverware",
            "d_ethernet_connection",
            "d_extra_pillows_and_blankets",
            "d_fire_extinguisher",
            "d_first_aid_kit",
            "d_freezer",
            "d_hot_water_kettle",
            "d_keypad",
            "d_long_term_stays_allowed",
            "d_outdoor_furniture",
            "d_private_entrance",
            "d_rice_maker",
            "d_roomdarkening_shades",
            "d_safe",
            "d_shared_pool",
            "d_smart_lock",
            "d_smoke_alarm",
            "d_waterfront",
            "d_window_guards",
            "d_wine_glasses",
            "d_have_wifiinternet",
            "d_have_cable",
            "d_have_tv",
            "d_have_heating",
            "d_have_air_condfan",
            "d_have_balconyterrace",
            "d_have_breakfast",
            "d_have_childrenbabycribhighcornerchang",
            "d_have_fireplace",
            "d_have_clothing_storage")

basic_lev  <- c("f_property_type", "n_accommodates", "n_beds")


# Factorized variables
basic_add <- c("f_bathrooms", "f_bedrooms", "f_neighbourhood_cleansed", "f_minimum_nights", "n_availability_365")


reviews <- c("n_review_scores_rating",
             "f_review_scores_rating",
             "n_number_of_reviews",
             "n_reviews_per_month")

host <- c("d_host_identity_verified",
          "d_host_greets_you")



# Building OLS models

model1 <- " ~ n_accommodates"
model2 <- paste0(" ~ ",paste(basic_lev,collapse = " + "))
model3 <- paste0(" ~ ",paste(c(basic_lev, basic_add, reviews, host, dummies ),collapse = " + "))


m1 <-  "~ n_accommodates"
m2 <- "~ n_accommodates+ basic_lev"
m3 <- "~ n_accommodates+ basic_lev + basic_add + reviews + host + dummies"
m4<- "~ n_accommodates+ basic_lev + basic_add + reviews + host + dummies + interaction"

  
  
model_variables <- c( m1,m2,m3,m4)
model_names <- c("M1", "M2", "M3","M4")
model_table <- as.data.frame(cbind(model_names, model_variables))
model_headings <- c("Model", "Predictor Variables")
colnames(model_table) <- model_headings


# Do the iteration


for ( i in 1:4 ){
  print(paste0( "Estimating model: " ,i ))
  # Get the model name
  model_name <-  paste0("model",i)
  model_pretty_name <- paste0("M",i,"")
  # Specify the formula
  yvar <- "price"
  xvars <- eval(parse(text = model_name))
  formula <- formula(paste0(yvar,xvars))
  
  # Estimate model on the whole sample
  model_work_data <- feols( formula , data = data_train , vcov='hetero' )
  #  and get the summary statistics
  fs  <- fitstat(model_work_data,c('rmse','r2','bic'))
  BIC <- fs$bic
  r2  <- fs$r2
  rmse_train <- fs$rmse
  ncoeff <- length( model_work_data$coefficients )
  
  # Do the k-fold estimation
  set.seed(seed_val)
  cv_i <- train( formula, data_train, method = "lm", 
                 trControl = trainControl(method = "cv", number = k_folds))
  rmse_test <- mean( cv_i$resample$RMSE )
  
  # Save the results
  model_add <- tibble(Model=model_pretty_name, Coefficients=ncoeff,
                      R_squared=r2, BIC = BIC, 
                      Training_RMSE = rmse_train, Test_RMSE = rmse_test )
  if ( i == 1 ){
    model_results <- model_add
  } else{
    model_results <- rbind( model_results , model_add )
  }
}

# Check summary table
# Add it to final results

model_results <- rbind( model_results , lasso_add )
model_results

## The purpose of model4 was primarily to include all the relevant variables and use it in LASSO to identify predictors with non-zero coefficients.##

predictors_model3 <- c(basic_lev,basic_add, reviews, host, dummies)
set.seed(200)
system.time({
  ols_model <- train(
    formula(paste0("price ~", paste0(predictors_model3, collapse = " + "))),
    data = data_train,
    method = "lm",
    trControl = train_control
  )
})
ols_model_coeffs <-  ols_model$finalModel$coefficients
ols_model_coeffs_df <- data.frame(
  "variable" = names(ols_model_coeffs),
  "ols_coefficient" = ols_model_coeffs
) %>%
  mutate(variable = gsub("`","",variable))


```

```{r message=FALSE, warning=FALSE, include=FALSE}
##################
## Random Forest##
##################

predictors <- c(basic_lev, basic_add, host, reviews, dummies ,interactions)

# set tuning 
tune_grid <- expand.grid(
  .mtry = c(8, 10, 12),
  .splitrule = "variance",
  .min.node.size = c(5, 10, 15)
)

set.seed(2000)

system.time({
  rf_model <- train(
    formula(paste0("price ~", paste0(predictors, collapse = " + "))),
    data = data_train,
    method = "ranger",
    trControl = train_control,
    tuneGrid = tune_grid,
    importance = "impurity"
  )
})
rf_model


rf_tuning_model_table <- rf_model$results %>%
  dplyr::select(mtry, min.node.size, RMSE) %>%
  dplyr::rename(nodes = min.node.size) %>%
  spread(key = mtry, value = RMSE)

# auto tuning

set.seed(110)
system.time({
  rf_model_auto <- train(
    formula(paste0("price ~", paste0(predictors, collapse = " + "))),
    data = data_train,
    method = "ranger",
    trControl = train_control,
    importance = "impurity"
  )
})
rf_model_auto



```

```{r message=FALSE, warning=FALSE, include=FALSE}
##Variable Importance Plots Rf_model ##

rf_model_var_imp <- ranger::importance(rf_model$finalModel)/1000

rf_model_var_imp_df <-
  data.frame(varname = names(rf_model_var_imp),imp = rf_model_var_imp) %>%
  mutate(varname = gsub("f_neighbourhood_cleansed", "Neighbourhood:", varname) ) %>%
  mutate(varname = gsub("f_property_type", "Property type:", varname) ) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))

rf_model_var_imp_df

# to have a quick look

plot(varImp(rf_model))

# have a version with top 10 vars only
ggplot(rf_model_var_imp_df[1:10,], aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='red', size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='red', size=0.75) +
  ylab("Importance (Percent)") +
  xlab("Variable Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw()

##############################
## 2) var_imp plot grouped ####
##############################

# grouped variable importance - keep binaries created off factors together

varnames <- rf_model$finalModel$xNames

f_neighbourhood_cleansed_varnames <- grep("f_neighbourhood_cleansed",varnames, value = TRUE)
f_host_varnames <- grep("d_host",varnames, value = TRUE)
f_property_type_varnames <- grep("f_property_type",varnames, value = TRUE)
f_reviews_varnames <- grep("review",varnames, value = TRUE)

a <- grep("d_",varnames, value = TRUE)
b <- a[135:354]

dummies_varnames <- b

groups <- list(host=f_host_varnames,
               property_type = f_property_type_varnames,
               reviews = f_reviews_varnames,
               neighbourhood=f_neighbourhood_cleansed_varnames,
               Ammenities = dummies_varnames,
               bathroom = "f_bathrooms",
               minimum_nights = "f_minimum_nights",
               n_accommodates = "n_accommodates",
               availability_365="n_availability_365",
               n_beds = "n_beds")

# Need a function to calculate grouped var-imp

group.importance <- function(rf.obj, groups) {
  var.imp <- as.matrix(sapply(groups, function(g) {
    sum(ranger::importance(rf.obj)[g], na.rm = TRUE)
  }))
  colnames(var.imp) <- "MeanDecreaseGini"
  return(var.imp)
}

rf_model_var_imp_grouped <- group.importance(rf_model$finalModel, groups)
rf_model_var_imp_grouped_df <- data.frame(varname = rownames(rf_model_var_imp_grouped),
                                          imp = rf_model_var_imp_grouped[,1])  %>%
  mutate(imp_percentage = imp/sum(imp))

##Showing the importance of features according to the grouped variables
ggplot(rf_model_var_imp_grouped_df, aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='red', size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='red', size=0.7) +
  ylab("Importance (Percent)") +   xlab("Variable Name") +
  coord_flip() +
  # expand=c(0,0),
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw()


##Variable Importance Plots rf_model_auto

rf_model_auto_var_imp <- ranger::importance(rf_model_auto$finalModel)/1000
rf_model_auto_var_imp_df <-
  data.frame(varname = names(rf_model_auto_var_imp),imp = rf_model_var_imp) %>%
  mutate(varname = gsub("f_neighbourhood_cleansed", "Neighbourhood:", varname) ) %>%
  mutate(varname = gsub("f_property_type", "Property type:", varname) ) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))
rf_model_auto_var_imp_df

# to have a quick look

plot(varImp(rf_model_auto))

# have a version with top 10 vars only

x <- ggplot(rf_model_auto_var_imp_df[1:10,], aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='red', size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='red', size=0.75) +
  ylab("Importance (Percent)") +
  xlab("Variable Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw()

##############################
# 2) varimp plot grouped

##############################
# grouped variable importance - keep binaries created off factors together

varnames_auto <- rf_model_auto$finalModel$xNames

f_neighbourhood_cleansed_varnames_auto <- grep("f_neighbourhood_cleansed",varnames, value = TRUE)
f_host_varnames_auto <- grep("d_host",varnames_auto, value = TRUE)
f_property_type_varnames_auto <- grep("f_property_type",varnames_auto, value = TRUE)
f_reviews_varnames_auto <- grep("review",varnames_auto, value = TRUE)
dummies_varnames_auto <- b




groups_auto <- list(host=f_host_varnames_auto,
                    property_type = f_property_type_varnames_auto,
                    reviews = f_reviews_varnames_auto,
                    neighbourhood=f_neighbourhood_cleansed_varnames_auto,
                    Ammenities = dummies_varnames_auto,
                    bathroom = "f_bathrooms",
                    minimum_nights = "f_minimum_nights",
                    n_accommodates = "n_accommodates",
                    availability_365="n_availability_365",
                    n_beds = "n_beds")


# Need a function to calculate grouped var-imp

group.importance <- function(rf.obj, groups_auto) {
  var.imp <- as.matrix(sapply(groups, function(g) {
    sum(ranger::importance(rf.obj)[g], na.rm = TRUE)
  }))
  colnames(var.imp) <- "MeanDecreaseGini"
  return(var.imp)
}

rf_model_auto_var_imp_grouped <- group.importance(rf_model_auto$finalModel, groups)
rf_model_auto_var_imp_grouped_df <- data.frame(varname = rownames(rf_model_auto_var_imp_grouped),
                                               imp = rf_model_auto_var_imp_grouped[,1])  %>%
  mutate(imp_percentage = imp/sum(imp))

u <- ggplot(rf_model_auto_var_imp_grouped_df, aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='red', size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='red', size=0.7) +
  ylab("Importance (Percent)") +   xlab("Variable Name") +
  coord_flip() +
  # expand=c(0,0),
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw()



# evaluate random forests 

results <- resamples(
  list(
    model_1  = rf_model,
    model_auto  = rf_model_auto
  )
)
summary(results)


```


```{r message=FALSE, warning=FALSE, include=FALSE}
# CART with pruning

# CART with built-in pruning

set.seed(200)
system.time({
  cart_model <- train(
    formula(paste0("price ~", paste0(predictors, collapse = " + "))),
    data = data_train,
    method = "rpart",
    tuneLength = 10,
    trControl = train_control
  )
})

cart_model

library(rpart)
library(rpart.plot)

# Tree graph

rpart.plot(cart_model$finalModel, tweak=1.2, digits=-1, extra=1)

```


```{r message=FALSE, warning=FALSE, include=FALSE}
# GBM

gbm_grid <-  expand.grid(interaction.depth = 10, # complexity of the tree
                         n.trees = 250, # number of iterations, i.e. trees
                         shrinkage = 0.1, # learning rate: how quickly the algorithm adapts
                         n.minobsinnode = 20 # the minimum number of training set samples in a node to commence splitting
)

set.seed(111)
system.time({
  gbm_model <- train(formula(paste0("price ~", paste0(predictors, collapse = " + "))),
                     data = data_train,
                     method = "gbm",
                     trControl = train_control,
                     verbose = FALSE,
                     tuneGrid = gbm_grid)
})
gbm_model
gbm_model$finalModel
save( gbm_model , file = 'gbm_model.RData' )

```


```{r message=FALSE, warning=FALSE, include=FALSE}
# get prediction rmse and add to next summary table
# ---- compare these models

final_models <-
  list("OLS" = ols_model,
       "CART" = cart_model,
       "Random forest 1: Tuning provided" = rf_model,
       "Random forest 2: Auto Tuning" = rf_model_auto,
       "GBM"  = gbm_model)
results <- resamples(final_models) %>% summary()
results

# Model selection is carried out on this CV RMSE

result <- imap(final_models, ~{
  mean(results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("CV RMSE" = ".")
result


# evaluate preferred model on the holdout set -----------------------------
result_2 <- map(final_models, ~{
  RMSE(predict(.x, newdata = data_test), data_test[["price"]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("Holdout RMSE" = ".")
result_2

```

```{r message=FALSE, warning=FALSE, include=FALSE}
#########################################################################################
# Partial Dependence Plots for the best model; random forest with auto tuning
#########################################################################################

# 1) Property Type
pdp_f_property_type <- pdp::partial(rf_model_auto, pred.var = "f_property_type", 
                                    pred.grid = distinct_(data_test, "f_property_type"), 
                                    train = data_train)
fig22 <- pdp_f_property_type %>%
  autoplot( ) +
  geom_point(color='red', size=2) +
  geom_line(color='red', size=1) +
  ylab("Predicted price") +
  xlab("Property Type") +
  theme_bw()

# 2) Number of accommodates
pdp_n_accommodates <- pdp::partial(rf_model_auto, pred.var = "n_accommodates", 
                                   pred.grid = distinct_(data_test, "n_accommodates"), 
                                   train = data_train)
fig23 <- pdp_n_accommodates %>%
  autoplot( ) +
  geom_point(color='red', size=4) +
  ylab("Predicted price") +
  xlab("Accommodates (persons)") +
  theme_bw()


```


```{r message=FALSE, warning=FALSE, include=FALSE}
####
# Sub-sample performance: RMSE / mean(y) ---------------------------------------
# NOTE  we do this on the holdout set.
# 

data_holdout_w_prediction <- data_test %>%
  mutate(predicted_price = predict(rf_model_auto, newdata = data_test))

######### create nice summary table of heterogeneity

a <- data_holdout_w_prediction %>%
  mutate(is_low_size = ifelse(n_accommodates <= 3, "small apt", "large apt")) %>%
  group_by(is_low_size) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price),
    mean_price = mean(price),
    rmse_norm = RMSE(predicted_price, price) / mean(price)
  )

unique(toronto_final$n_beds)

b <- data_holdout_w_prediction %>%
  filter(n_beds %in% c("1","2","3", "4")) %>%
  group_by(n_beds) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price),
    mean_price = mean(price),
    rmse_norm = rmse / mean_price
  )

c <- data_holdout_w_prediction %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price),
    mean_price = mean(price),
    rmse_norm = RMSE(predicted_price, price) / mean(price)
  )

unique(toronto_final$f_property_type)
d <- data_holdout_w_prediction %>%
  filter(f_property_type %in% c("Apartment", "Condo","Loft")) %>%
  group_by(f_property_type) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price),
    mean_price = mean(price),
    rmse_norm = rmse / mean_price
  )

## Save output ##

colnames(a) <- c("", "RMSE", "Mean price", "RMSE/price")
colnames(b) <- c("", "RMSE", "Mean price", "RMSE/price")
c<- cbind("All", c)
colnames(d) <- c("", "RMSE", "Mean price", "RMSE/price")

line1 <- c("Apartment size", "", "", "")
line2 <- c("Beds", "", "", "")
line3 <- c("Property Type", "", "", "")

result_3 <- rbind(line1,a,line2, b,line3,d) %>%
  transform(RMSE = as.numeric(RMSE), `Mean price` = as.numeric(`Mean price`),
            `RMSE/price` = as.numeric(`RMSE/price`))
result_3


```


```{r message=FALSE, warning=FALSE, include=FALSE}
# FIGURES FOR FITTED VS ACTUAL OUTCOME VARIABLES #
##--------------------------------------------------

Ylev <- data_test[["price"]]

# Predicted values
prediction_test_pred <- as.data.frame(predict(rf_model_auto, newdata = data_test, interval="predict"))

predictionlev_test <- cbind(data_test[,c("price","n_accommodates")],
                               prediction_test_pred)
colnames(d)


# Create data frame with the real and predicted values
d <- data.frame(ylev=Ylev, predlev=predictionlev_test[,3] )
# Check the differences
d$elev <- d$ylev - d$predict.rf_model_auto..newdata...data_test..interval....predict..

# Plot predicted vs price
level_vs_pred <- ggplot(data = d) +
  geom_point(aes(y=ylev, x=predict.rf_model_auto..newdata...data_test..interval....predict..), color = "purple", size = 1,
             shape = 16, alpha = 0.5, show.legend=FALSE, na.rm=TRUE) +
  geom_segment(aes(x = 0, y = 0, xend = 275, yend =275), size=0.8, color="black", linetype=2) +
  labs(y = "Price (US dollars)", x = "Predicted price  (US dollars)") +
  theme_bg()



```



## Executive Summary

The purpose of this project was to help a company in setting their price for their newly launched apartments in Toronto, Canada. This company operates and deals with small and mid-sized apartments hosting 2-6 guests. There were 4 models that were created, namely OLS Linear Regression, Random Forest (parameters provided & Auto-Tuning), Cart & GBM, and their performance was evaluated based on their RMSE. The best model that turned out to be was Random Forest with Auto-Tuning.

## Data Selection

The data set that I initially downloaded from the Airbnb website contained 15,435 listings, however it had to be cleaned and filtered down to only the listings that were relevant and similar to the apartments that my company was about to launch, making sure that the data we used in our models was a result of conscious decisions made, ensuring that we were not feeding any irrelevant data into the models that made us compromise on the quality and accuracy of the models. Hence, a lot of time and detailed thought process was spent on the data cleaning and data preparation. 
The listings in the data set contained all type of listings including the ones that were not apartments or were not similar to apartments, like private suits, so I filtered the data down to the ones that were relevant, namely, “Entire loft", "Entire serviced apartment", "Entire home/apt" and "Entire condominium (condo)”. Even from those listings I filtered down to the ones that accommodated 2 to 6 people, as these were the only ones that matched our criteria. I also removed the percentage signs from the percentage columns such as the host acceptance rate and the host response rate. The dollar sign was also removed from the price variable, which is our target variable. After that I changed the values in the property type to make them cleaner and remove the redundant words such as “Entire condominium (condo)" was changed to "Condo".  

```{r echo=FALSE, fig.height=3, fig.width=8, message=FALSE, warning=FALSE}

viz1
```

## Data Engineering

There was only one listing that was scraped on the 9th of January, so I just dropped it. After that I started performing basic data checks and the first one was to check if data had rows with only NAs and duplicate values, as it made no sense to keep them. However, I did not find any such rows. Another check that I performed was to see how many columns had more than 50% NA values, to figure out which variables needed more attention, and drop them especially if they are not important predictors. There were 3 variables that had all the values as NAs, including the “bathrooms”, so I just dropped them right away. However, Bathroom is an important feature for predicting the apartment price, so I extracted that information as a numeric variable from a column named bathroom_text. I also dropped all the other irrelevant variables at this stage as well. Another important factor while valuing the property is the kind of amenities an apartment comes with. In the initial data set, the amenities were included in one variable as a list, and for us to be able to include all those features in the models, it was necessary to extract all of them into separate dummy variables, and for that I had to go through extensive data cleaning and preparation. There were a lot of amenities, even those that did not make much of a difference. So, after extracting all the amenities, I dropped the ones that had minimal and no impact and clubbed those which were quite similar to each other. For instance, TV and Netflix, and children, crib, and baby were clubbed into one separate dummy variable. For this process, the domain knowledge was extremely important, and I had to go through extensive research. 

There was no point including variables that had no variance in them, as they would not help predicting the price at all, one example was the room type variable. The filtered data had only one type of room, so I dropped that variable. As mentioned earlier, the number of bathrooms can make a difference in the property value and there were a couple of conscious decisions that I made while cleaning this variable. The data set included bathroom values in decimals as well, that do not make much sense and for easier and clearer interpretation, I rounded them to whole number. For instance, 1.5, 2.5 and 3.5 bathrooms were just considered to be 2,3 and 4 respectively, the rationale behind this was that I considered the half bathroom as a whole bathroom as well. 

For all the variables that I converted to factors, percentages, numeric and dummy, I put a prefix of f, p, n and d respectively so that I can easily recognize and then only kept those, apart form the target variable, as these were the final variables that were going to be used to make the predictive models. One important decision that I had to make was that how the missing values were going to be dealt with. There was a different approach taken for different variable, based on my domain knowledge. For instance, for the variable missing bed, I used the variable, number of accommodates, to apply a sense check. Where there were 3 people accommodated, I included number of beds as 2, and this is the ratio that was applied throughout. 

I then checked the distribution of our target variable (Price), to see if there was a need to transform it my taking a log of it to make it more normally distributed. However, upon inquiry I found out that there was no need for taking the log and absolute values resembled more to that of a normal distribution. I also filtered down to the apartments that were priced at 350 dollars or less, since it was very rare for the apartments to be priced above that and since our apartments are going to small or mid-sized, there was no need to include the ones above 350 dollars. Finally, for the cleaning and preparation phase I pooled values within certain variables, which did not have a significant difference between their mean values, before converting them as factors. 

After all the data cleaning and data preparation, following are the variables that were used in the models that were created to predict the price of an apartment:

- *Dummies*: Binary variables consisting of all the amenities that are being offered by host. 
- *Size variables*: This includes numeric variable like number of beds, number of baths, number of people it accommodates, and minimum nights. 
- *Factor variables*: For each Neighbourhood, type of property, including flag and factorized variable of size variables.  
- *Reviews variables*: Review score rating and the number of reviews the apartment gets each month. 
- *Host variables*: Dummies for host verification and if they a super host or not. 
- *Interaction Terms*

## Prediction

After all the data cleaning there were 3685 listings left. Of these listings, 20% were randomly selected as a holdout set (test set), while the rest were used as a training set. The training set underwent 5 test fold cross validation, allowing our models to perform more efficiently and provide the coefficients with least amount of overfitting. The result of these cross-validated RMSEs were used to then evaluate the model performance and was used as primary criteria to select the best model. 

We ran a total of 4 different prediction models: a simple Ordinary Least Squared (OLS) model containing basic variables, Classification and Regression Tree (CART) with pruning, two Random Forest (RF) models where 1 was provided with the tuning parameters and the other was run on automatic tunning and lastly a Gradient Boosting Machine (GBM) model.  
The results of the cross-validated Root Mean Squared Error (RMSE) are provided in this adjacent table. It can be seen that the best model is Random Forest with Auto-tuning.


```{r echo=FALSE, message=FALSE, warning=FALSE}
result %>% kbl(caption = "Ranking of Models CV RSME", escape = FALSE) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling( position = "center")
```


I also made Partial Dependence Plots where it keeps everything else other than the predictive variable, we want to see association of with the target variable. We created two such graphs, one with the property type and the second one with the number of people it accommodates. In the property type, Lofts were the ones which were priced the highest while for the number of people it accommodates, price went up when the persons accommodated increased. For variable importance plots, we grouped together similar variables and re-calculated their importance to gauge the relative importance of these variable groups in predicting the prices. For our best model it showed that the amenities were the most important (almost 40%) and the property type and reviews were the next most important variables (almost 20%).


```{r echo=FALSE, fig.height=3, fig.width=8, message=FALSE, warning=FALSE}
fig7 <- ggarrange(
  fig22,
  fig23,
  nrow = 1)
fig7
```


```{r echo=FALSE, fig.height=3, fig.width=8, message=FALSE, warning=FALSE}
fig6<- ggarrange(
  x,
  u,
  nrow = 1)
fig6
```


## Conclusion
 
The best predictive model was Random Forest with Auto Tuning, while in the case study it was Random Forest with tuning parameters. We could have shortlisted the same if we wanted our model to be more time efficient as the difference between their RMSEs was minimal, and Auto-tuning takes a lot of time. But we still went with Auto-tuning as we only wanted to choose the best model in terms of the least error ($48.7 – The possible error in predicting the price). The recommendation to the company according to the final results is that they should invest in lofts in the Water-Front community, which is loaded with at least the basic amenities (the more the better), with the option of accommodating the maximum possible number of persons, as it will allow them to price their listings the highest and potentially earn highest profit. The RMSE of our model, compared to the one is higher, however there are lot of factors that have changed over time. For instance, the data in the case study was from 2017 and our data is from 2022, and one potential change is the variety and range of apartments being offered.

## Predicted vs actual prices

```{r echo=FALSE, message=FALSE,warning=FALSE,show.fig="hold",fig.width=8,fig.height=3,fig.pos="centre"}
level_vs_pred
```

